#coding=utf8
import time
import re
import jieba.analyse
import os
import math
import json
import datetime

"""
åŠŸèƒ½:
    ç»™å®šè¯é¢˜äº§ç”Ÿå…¶ä¿¡å·
å‚æ•°ï¼š
    topicï¼š           è¯é¢˜
    based_point:      åŸºå‡†æ—¶é—´ç‚¹ï¼ˆå¯¹çƒ­ç‚¹è¯é¢˜å‚è€ƒä¿¡å·ä¸ºå…¶æˆä¸ºçƒ­ç‚¹çš„æ—¶é—´ç‚¹ï¼Œå¯¹äºéçƒ­ç‚¹è¯é¢˜éšæœºé€‰æ‹©ä¸€ä¸ªæ—¶é—´ç‚¹ï¼‰
    start_point:      ä¿¡å·èµ·å§‹æ—¶é—´ç‚¹
    end_point:        ä¿¡å·ç»“æŸæ—¶é—´ç‚¹
    
    signal_length:    ä¿¡å·é•¿åº¦
    signal_span:      æ¯ä¸ªä¿¡å·çš„æ—¶é—´è·¨åº¦  
    
    blog_filepath:    å­˜æ”¾å¾®åšçš„æ–‡ä»¶è·¯å¾„
"""
class GenerateTopicSignals():

    def __init__(self,topic=None):
        pass

    #å¯¹åŸå§‹ä¿¡å·è¿›è¡Œå¤„ç†
    #1ã€baseline normalization å‚æ•°ï¼šbeta
    #2ã€spike normalization å‚æ•°ï¼š alpha
    #3ã€smothing å‚æ•°ï¼šN_smooth
    def process_signals(self,original_signal,beta,alpha,N_smooth,b_flag=True):

        #step1:baseline normalization
        if b_flag:
            b=sum(original_signal)
            signals_b =[(s/(float(b)+0.0001))**beta for s in original_signal]
        else:
            signals_b=original_signal

        #step2:spike normalization
        signals_b_s=[float(abs(signals_b[i]-signals_b[i-1]))**alpha for i in range(1,len(original_signal))]
        # signals_b_sçš„é•¿åº¦è¾ƒåŸå§‹ä¿¡å·é•¿åº¦å‡1
        signals_b_s.insert(0,signals_b[0])  #è¡¥é½ï¼Ÿï¼Ÿï¼Ÿï¼ˆå¯èƒ½æœ‰é—®é¢˜ï¼‰

        #step3:smothing
        signals_b_s_c=[]
        signals_b_s_c_l=[]
        for i in range(1, len(signals_b_s)):
            sum_smooth = 0
            for j in range(i - N_smooth, i):
                if j < 0:
                    continue
                sum_smooth += signals_b_s[j]
            signals_b_s_c.append(sum_smooth)
            signals_b_s_c_l.append(math.log(sum_smooth+0.00001))
        #signals_b_s_cå’Œsignals_b_s_c_lä¿¡å·é•¿åº¦è¾ƒsignals_b_sé•¿åº¦å‡1
        signals_b_s_c.insert(0,signals_b_s[0])  #è¡¥é½ï¼Ÿï¼Ÿï¼Ÿï¼ˆå¯èƒ½æœ‰é—®é¢˜ï¼‰
        signals_b_s_c_l.insert(0,math.log(signals_b_s[0]+0.00001)) #è¡¥é½ï¼Ÿï¼Ÿï¼Ÿï¼ˆå¯èƒ½æœ‰é—®é¢˜ï¼‰

        print len(original_signal),':',original_signal
        print len(signals_b),':',signals_b
        print len(signals_b_s),':',signals_b_s
        print len(signals_b_s_c),':',signals_b_s_c
        print len(signals_b_s_c_l),':',signals_b_s_c_l

        return signals_b,signals_b_s,signals_b_s_c,signals_b_s_c_l

    #æ ¹æ®è¯é¢˜å…³è”çš„å¾®åšçš„å‘å¸ƒæ—¶é—´å½¢æˆè¯é¢˜ä¿¡å·ï¼Œè¿”å›
    #start_pointï¼šä»¥ç§’ä¸ºå•ä½çš„æµ®ç‚¹ç±»å‹ï¼Œè¯é¢˜ä¿¡å·èµ·å§‹æ—¶é—´ç‚¹
    #signal_spanï¼šä»¥ç§’ä¸ºå•ä½ï¼Œæ¯ä¸ªä¿¡å·çš„æ—¶é—´è·¨åº¦
    def generate_signals(self,blog_lst,start_point,signal_length,signal_span):

        #åˆå§‹åŒ–å‚è€ƒä¿¡å·
        signal = [0] * signal_length
        if len(blog_lst)==0:
            return signal
        #å¯¹äºè¯é¢˜ç›¸å…³çš„æ¯æ¡å¾®åš
        for blog_dict in blog_lst:
            blog_create_time=blog_dict['blog_create_time']
            #è®¡ç®—å¾®åšè½åœ¨ç¬¬å‡ ä¸ªä¿¡å·å†…
            i = int(float(blog_create_time - start_point) / signal_span)    #è¡¨ç¤ºå¾®åšè½åœ¨ç¬¬iä¸ªä¿¡å·å†…ï¼Œ
            #å¦‚æœå¾®åšè½åœ¨ä¿¡å·é•¿åº¦ä¹‹å¤–ï¼Œè·³å‡ºå¾ªç¯
            if i >= signal_length or i < 0:
                continue
            #å¦‚æœå¾®åšè½åœ¨ä¿¡å·é•¿åº¦ä¹‹å†…ï¼Œåˆ™å¯¹åº”çš„ä¿¡å·åŠ 1
            signal[i]+=1
        return signal

    #ç»™å®šè¯é¢˜ï¼Œåœ¨ä¸€ä¸ªå¾®åšæ–‡ä»¶ä¸­æ”¶é›†å…¶ç›¸å…³çš„å¾®åš,è¿”å›å¾®åšåˆ—è¡¨[{'blog_create_time':float,'blog_text':str}â€¦â€¦],å¯èƒ½ä¸ºç©º
    #blog_filenameå½¢å¼ä¸ºï¼š'/data0/shaojie5/source_mblog/data/mblog/20171228/25239839'
    def collect_blogs_for_one_topic_from_file(self,topic,blog_filename,start_point,end_point):

        print "ä¸‹è½½æ–‡ä»¶ä¸ºï¼š",blog_filename
        blogs_lst=[]
        for line in open(blog_filename).readlines():

            line = line.decode('utf8')
            items = line.strip().strip('\n').split('\t')

            content_dic = eval(items[7])            # å°†å­—ç¬¦ä¸²è½¬åŒ–ä¸ºå­—å…¸ç±»å‹
            timestr = content_dic['created_at']     # æå–å‡ºå¾®åšå‘å¸ƒçš„æ—¶é—´ï¼Œå­—ç¬¦ä¸²å½¢å¼å¦‚â€œSun Dec 17 23:58:49 +0800 2017â€
            # å¯¹å¾®åšå‘å¸ƒæ—¶é—´è¿›è¡Œå¤„ç†,å°†â€œSun Dec 17 23:58:49 +0800 2017â€è½¬åŒ–ä¸º#è½¬åŒ–ä¸ºæ—¶é—´å¯¹è±¡ï¼Œç”¨ç§’æ•°æ¥è¡¨ç¤ºæ—¶é—´çš„æµ®ç‚¹æ•°
            time1 = timestr[4:7]    # æå–å‡ºæœˆä»½ï¼Œå½¢å¼å¦‚ï¼šâ€˜Decâ€™
            time_dic = {
                'Jan': '1',
                'Feb': '2',
                'Mar': '3',
                'Apr ': '4',
                'May': '5',
                'Jun': '6',
                'Jul': '7',
                'Aug': '8',
                'Sep': '9',
                'Oct': '10',
                'Nov': '11',
                'Dec': '12'
            }        #æœˆä»½è¯å…¸
            time2 = timestr[8:16]   # æå–å‡ºå¤©ï¼Œå°æ—¶ï¼Œåˆ†é’Ÿï¼Œå½¢å¼å¦‚ï¼šâ€˜17 23:58â€™
            year=timestr[26:]
            # time_last = '2017-' + time_dic[time1] + '-' + time2  # å¾—åˆ°å¾®åšå‘å¸ƒæ—¶é—´ï¼Œå­—ç¬¦ä¸²ç±»å‹å½¢å¼å¦‚â€œ2017-12-17 23:58â€
            time_last=year+'-'+time_dic[time1] + '-' + time2
            blog_create_time = time.mktime(time.strptime(time_last, '%Y-%m-%d %H:%M'))  # è½¬åŒ–ä¸ºæ—¶é—´å¯¹è±¡ï¼Œç”¨ç§’æ•°æ¥è¡¨ç¤ºæ—¶é—´çš„æµ®ç‚¹æ•°
            # print "å¾®åšåˆ›å»ºæ—¶é—´ä¸ºï¼š",time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(blog_create_time)),'==',blog_create_time
            # print "èµ·å§‹å’Œç»ˆæ­¢æ—¶é—´ä¸ºï¼š",start_point , end_point
            #åˆ¤æ–­å¾®åšå‘å¸ƒæ—¶é—´æ˜¯å¦åœ¨æ‰€éœ€ä¿¡å·æ—¶é—´æ®µå†…
            if blog_create_time>=start_point and blog_create_time<=end_point:
                text = content_dic['text']  # æå–å‡ºå¾®åšçš„æ–‡æœ¬å†…å®¹,unicodeæ ¼å¼
                #å¯¹å¾®åšæ–‡æœ¬è¿›è¡Œæ¸…æ´—
                blog_text = self.filter_symbol(text)
                # print blog_text
                # è·å–å¾®åšæ­£æ–‡ä¸­çš„å…³é”®è¯ï¼Œå‰10ä¸ª
                # key_words_list = jieba.analyse.extract_tags(blog_text, topK=20, withWeight=False,
                #                                             allowPOS=(("nr", "nr1", "nr2", "ns", "n", "nsf", "nt", "nrf")))
                key_words_list = jieba.analyse.extract_tags(blog_text, topK=10, withWeight=False)
                #è®¾ç½®è®¡æ•°å™¨
                count = 0
                if len(key_words_list)<=3:
                    continue
                # éå†å¾®åšå…³é”®è¯åˆ—è¡¨
                for keyword in key_words_list:
                    # å¦‚æœè¯é¢˜ä¸­åŒ…å«å¾®åšä¸­çš„å…³é”®è¯ï¼Œè®¡ç®—åŠ 1
                    if topic.__contains__(keyword.encode('utf-8')):
                        count += 1
                    # å¦‚æœå¾®åšä¸­2ä¸ªä»¥ä¸Šçš„å…³é”®è¯åœ¨è¯é¢˜ä¸­ï¼Œåˆ™å¾®åšå±äºè¯¥è¯é¢˜
                    if count >= 2:
                        dict={}
                        dict['blog_create_time']=blog_create_time                   #æµ®ç‚¹å‹
                        dict['blog_text']=blog_text.encode('utf-8')                 #å­—ç¬¦ä¸²ï¼Œè½¬åŒ–ä¸ºutf-8æ ¼å¼
                        print "ä¿¡å·å¼€å§‹æ—¶é—´ç‚¹:", start_point, '==', time.strftime("%Y-%m-%d %H:%M:%S",
                                                                           time.localtime(start_point))
                        # print time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(blog_create_time)), ':', blog_text
                        # print time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(blog_create_time)), ':', ' '.join(
                        #     key_words_list)
                        print time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(blog_create_time)), ':', blog_text.encode('utf-8')

                        print "ä¿¡å·ç»“æŸæ—¶é—´ç‚¹:", end_point, '==', time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(end_point))
                        #print blog_create_time,':',blog_text
                        print count
                        blogs_lst.append(dict)
                        break
        return blogs_lst

    def collect_blogs_for_one_topic_from_file_v1(self,topic,blog_filename,start_point,end_point):

        print "ä¸‹è½½æ–‡ä»¶ä¸ºï¼š",blog_filename
        blogs_lst=[]
        for line in open(blog_filename).readlines():

            line = line.decode('utf8')
            items = line.strip().strip('\n').split('\t')

            content_dic = eval(items[7])            # å°†å­—ç¬¦ä¸²è½¬åŒ–ä¸ºå­—å…¸ç±»å‹
            timestr = content_dic['created_at']     # æå–å‡ºå¾®åšå‘å¸ƒçš„æ—¶é—´ï¼Œå­—ç¬¦ä¸²å½¢å¼å¦‚â€œSun Dec 17 23:58:49 +0800 2017â€
            # å¯¹å¾®åšå‘å¸ƒæ—¶é—´è¿›è¡Œå¤„ç†,å°†â€œSun Dec 17 23:58:49 +0800 2017â€è½¬åŒ–ä¸º#è½¬åŒ–ä¸ºæ—¶é—´å¯¹è±¡ï¼Œç”¨ç§’æ•°æ¥è¡¨ç¤ºæ—¶é—´çš„æµ®ç‚¹æ•°
            time1 = timestr[4:7]    # æå–å‡ºæœˆä»½ï¼Œå½¢å¼å¦‚ï¼šâ€˜Decâ€™
            time_dic = {
                'Jan': '1',
                'Feb': '2',
                'Mar': '3',
                'Apr ': '4',
                'May': '5',
                'Jun': '6',
                'Jul': '7',
                'Aug': '8',
                'Sep': '9',
                'Oct': '10',
                'Nov': '11',
                'Dec': '12'
            }        #æœˆä»½è¯å…¸
            time2 = timestr[8:16]   # æå–å‡ºå¤©ï¼Œå°æ—¶ï¼Œåˆ†é’Ÿï¼Œå½¢å¼å¦‚ï¼šâ€˜17 23:58â€™
            year=timestr[26:]
            # time_last = '2017-' + time_dic[time1] + '-' + time2  # å¾—åˆ°å¾®åšå‘å¸ƒæ—¶é—´ï¼Œå­—ç¬¦ä¸²ç±»å‹å½¢å¼å¦‚â€œ2017-12-17 23:58â€
            time_last=year+'-'+time_dic[time1] + '-' + time2
            blog_create_time = time.mktime(time.strptime(time_last, '%Y-%m-%d %H:%M'))  # è½¬åŒ–ä¸ºæ—¶é—´å¯¹è±¡ï¼Œç”¨ç§’æ•°æ¥è¡¨ç¤ºæ—¶é—´çš„æµ®ç‚¹æ•°
            # print "å¾®åšåˆ›å»ºæ—¶é—´ä¸ºï¼š",time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(blog_create_time)),'==',blog_create_time
            # print "èµ·å§‹å’Œç»ˆæ­¢æ—¶é—´ä¸ºï¼š",start_point , end_point
            #åˆ¤æ–­å¾®åšå‘å¸ƒæ—¶é—´æ˜¯å¦åœ¨æ‰€éœ€ä¿¡å·æ—¶é—´æ®µå†…
            if blog_create_time>=start_point and blog_create_time<=end_point:
                text = content_dic['text']  # æå–å‡ºå¾®åšçš„æ–‡æœ¬å†…å®¹,unicodeæ ¼å¼
                #å¯¹å¾®åšæ–‡æœ¬è¿›è¡Œæ¸…æ´—
                blog_text = self.filter_symbol(text)
                # print blog_text
                # è·å–å¾®åšæ­£æ–‡ä¸­çš„å…³é”®è¯ï¼Œå‰10ä¸ª
                # key_words_list = jieba.analyse.extract_tags(blog_text, topK=20, withWeight=False,
                #                                             allowPOS=(("nr", "nr1", "nr2", "ns", "n", "nsf", "nt", "nrf")))
                key_words_list = jieba.analyse.extract_tags(blog_text, topK=10, withWeight=False)
                #è®¾ç½®è®¡æ•°å™¨
                count = 0
                if len(key_words_list)<=3:
                    continue

                # éå†å¾®åšå…³é”®è¯åˆ—è¡¨,åªè¦è¯é¢˜ä¸­åŒ…å«å¾®åšä¸­çš„ä¸€ä¸ªå…³é”®è¯å°±è¡Œ
                for keyword in key_words_list:
                    # å¦‚æœè¯é¢˜ä¸­åŒ…å«å¾®åšä¸­çš„å…³é”®è¯ï¼Œè®¡ç®—åŠ 1
                    if topic.__contains__(keyword.encode('utf-8')):
                    #     count += 1
                    # # å¦‚æœå¾®åšä¸­2ä¸ªä»¥ä¸Šçš„å…³é”®è¯åœ¨è¯é¢˜ä¸­ï¼Œåˆ™å¾®åšå±äºè¯¥è¯é¢˜
                    # if count >= 2:
                        dict={}
                        dict['blog_create_time']=blog_create_time                   #æµ®ç‚¹å‹
                        dict['blog_text']=blog_text.encode('utf-8')                 #å­—ç¬¦ä¸²ï¼Œè½¬åŒ–ä¸ºutf-8æ ¼å¼
                        print "ä¿¡å·å¼€å§‹æ—¶é—´ç‚¹:", start_point, '==', time.strftime("%Y-%m-%d %H:%M:%S",
                                                                           time.localtime(start_point))
                        # print time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(blog_create_time)), ':', blog_text
                        # print time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(blog_create_time)), ':', ' '.join(
                        #     key_words_list)
                        print time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(blog_create_time)), ':', blog_text.encode(
                            'utf-8')

                        print "ä¿¡å·ç»“æŸæ—¶é—´ç‚¹:", end_point, '==', time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(end_point))
                        #print blog_create_time,':',blog_text
                        print count
                        blogs_lst.append(dict)
                        break
        return blogs_lst
    #æ ¹æ®å¾®åšå‘å¸ƒæ—¶é—´å¯¹å¾®åšè¿›è¡Œè¿‡æ»¤
    #ç»™å®šè¯é¢˜åœ¨ä¸€ä¸ªå¾®åšæ–‡ä»¶å¤¹ä¸­æ”¶é›†å…¶ç›¸å…³çš„å¾®åšï¼Œè¿”å›å¾®åšåˆ—è¡¨[{'blog_create_time':float,'blog_text':str}â€¦â€¦]
    #blog_filepathå½¢å¼ä¸ºï¼š'/data0/shaojie5/source_mblog/data/mblog/20171228
    def collect_blogs_for_one_topic_from_fold(self,topic,blog_filepath,start_point,end_point):
        blog_lst=[]
        blog_file_lst=self.visitDir(blog_filepath=blog_filepath)
        for file in blog_file_lst:
            # # è·å–è¯¥æ–‡ä»¶æœ€åä¿®æ”¹æ—¶é—´ï¼ˆç±»å‹ä¸ºæµ®ç‚¹å‹ï¼‰ï¼Œå¹¶å‡å»180ç§’
            # t = os.path.getmtime(file) - 3 * 60  # ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
            # # å¦‚æœæ–‡ä»¶çš„æœ€åä¿®æ”¹æ—¶é—´åœ¨è¯é¢˜æˆä¸ºçƒ­ç‚¹æœŸé—´ï¼Œåˆ™ä½¿ç”¨å…¶ç”Ÿæˆå‚è€ƒä¿¡å·
            # if t < start_point and t > end_point:
            #     continue
            lst=self.collect_blogs_for_one_topic_from_file(topic=topic,
                                                           blog_filename=file,
                                                           start_point=start_point,
                                                           end_point=end_point)
            # lst = self.collect_blogs_for_one_topic_from_file_v1(topic=topic,
            #                                                  blog_filename=file,
            #                                                  start_point=start_point,
            #                                                  end_point=end_point)
            if len(lst)!=0:
                blog_lst.extend(lst)
        return blog_lst

    def collect_blogs_for_one_topic_from_fold_v1(self,topic,blog_filepath,start_point,end_point):
        blog_lst=[]
        blog_file_lst=self.visitDir(blog_filepath=blog_filepath)
        for file in blog_file_lst:
            # # è·å–è¯¥æ–‡ä»¶æœ€åä¿®æ”¹æ—¶é—´ï¼ˆç±»å‹ä¸ºæµ®ç‚¹å‹ï¼‰ï¼Œå¹¶å‡å»180ç§’
            # t = os.path.getmtime(file) - 3 * 60  # ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
            # # å¦‚æœæ–‡ä»¶çš„æœ€åä¿®æ”¹æ—¶é—´åœ¨è¯é¢˜æˆä¸ºçƒ­ç‚¹æœŸé—´ï¼Œåˆ™ä½¿ç”¨å…¶ç”Ÿæˆå‚è€ƒä¿¡å·
            # if t < start_point and t > end_point:
            #     continue
            lst=self.collect_blogs_for_one_topic_from_file_v1(topic=topic,
                                                           blog_filename=file,
                                                           start_point=start_point,
                                                           end_point=end_point)
            # lst = self.collect_blogs_for_one_topic_from_file_v1(topic=topic,
            #                                                  blog_filename=file,
            #                                                  start_point=start_point,
            #                                                  end_point=end_point)
            if len(lst)!=0:
                blog_lst.extend(lst)
        return blog_lst
    #æ ¹æ®æ–‡ä»¶ä¿®æ”¹æ—¶é—´å¯¹æ–‡ä»¶è¿‡æ»¤

    # éå†æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    def visitDir(self, blog_filepath):
        file_lst = []
        parents = os.listdir(blog_filepath)
        #å¯¹å…¶è¿›è¡Œæ’åº
        parents.sort()
        for parent in parents:
            # print parent
            child = os.path.join(blog_filepath, parent)
            file_lst.append(child)
            # print child
        print blog_filepath,'ä¸­æ–‡ä»¶æ•°ä¸ºï¼š',len(file_lst)
        return file_lst
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¯¹å¾®åšå†…å®¹æ¸…æ´—
    def filter_symbol(self,context):
        re_http = re.compile(r'[a-zA-z]+://[^\s]*')
        re_punc = re.compile('[\s+\.\!\/_,$%^*(+\"\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ğŸ™„â€œâ€ã€Šã€‹ã€ã€‘ï¼šï¼ˆï¼‰]+'.decode('utf8'))
        context = context.decode('utf-8')
        context = context.strip().strip('\n')
        context = re_http.sub('', context)
        context = re_punc.sub('', context)
        return context

"""
ä¸­é—´ç»“æœæ•°æ®ä¿å­˜ä¸å¯¼å…¥
"""
#ä¿å­˜
def store_json(data,filename):
    with open(filename, 'w') as json_file:
        # json_file.write(json.dumps(data))
        json.dump(data, json_file, indent=4, encoding='utf-8', ensure_ascii=False)
#å¯¼å…¥
def load_json(filename):
    with open(filename) as json_file:
        data = json.load(json_file)
    return data
#åˆ›å»ºæ–‡ä»¶ç›®å½•
def mkdir(path):
    # å»é™¤é¦–ä½ç©ºæ ¼
    path = path.strip()
    # å»é™¤å°¾éƒ¨ \ ç¬¦å·
    path = path.rstrip("\\")

    # åˆ¤æ–­è·¯å¾„æ˜¯å¦å­˜åœ¨
    # å­˜åœ¨     True
    # ä¸å­˜åœ¨   False
    isExists = os.path.exists(path)

    # åˆ¤æ–­ç»“æœ
    if not isExists:
        # å¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç›®å½•
        # åˆ›å»ºç›®å½•æ“ä½œå‡½æ•°
        os.makedirs(path)

        print path + ' åˆ›å»ºæˆåŠŸ'
        return True
    else:
        # å¦‚æœç›®å½•å­˜åœ¨åˆ™ä¸åˆ›å»ºï¼Œå¹¶æç¤ºç›®å½•å·²å­˜åœ¨
        print path + ' ç›®å½•å·²å­˜åœ¨'
        return False

"""
#ä¸ºçƒ­ç‚¹è¯é¢˜äº§ç”Ÿå‚è€ƒä¿¡å·
#blog_basepathçš„å½¢å¼ä¸ºï¼š'/data0/shaojie5/source_mblog/data/mblogâ€™
#blog_savepathä¸ºï¼šâ€˜pos_topic_related_blogsâ€™
#signal_savepathä¸ºï¼š'pos_topic_signals'
#topic_trended_timeä¸ºè¯é¢˜æˆä¸ºçƒ­ç‚¹æ—¶é—´ï¼ˆä¸ºå·²çŸ¥æ•°æ®ï¼Œæ¯ä¸ªå†å²çƒ­ç‚¹ä¼šæä¾›ï¼‰çš„å½¢å¼ï¼šâ€˜2017-12-28 13:43â€™
"""
def generate_refrence_signals_for_trended_topic(blog_basepath,
                                                blog_savepath,
                                                signal_savepath,
                                                trended_topic,
                                                topic_trended_time,
                                                topic_id,
                                                signal_span):

    N_ref1 = 7 * 3600  # è¯é¢˜æˆä¸ºçƒ­ç‚¹ä¹‹å‰çš„æ—¶é—´è·¨åº¦ï¼š7å°æ—¶(å¯ä¿®æ”¹)
    N_ref2 = 3 * 3600  # è¯é¢˜æˆä¸ºçƒ­ç‚¹ä¹‹åçš„æ—¶é—´è·¨åº¦ï¼š3å°æ—¶ï¼ˆå¯ä¿®æ”¹ï¼‰
    ref_signal_length = (N_ref1 + N_ref2) / signal_span  # å‚è€ƒä¿¡å·çš„é•¿åº¦
    print "å‚è€ƒä¿¡å·çš„é•¿åº¦ä¸ºï¼š",ref_signal_length

    trendTime = time.mktime(time.strptime(topic_trended_time, '%Y-%m-%d %H:%M'))  # è·å–è¯é¢˜æˆä¸ºçƒ­ç‚¹çš„æ—¶é—´æˆ³æµ®ç‚¹æ•°è¡¨ç¤º
    start_point = trendTime - N_ref1    # å‚è€ƒä¿¡å·å¼€å§‹æ—¶é—´ç‚¹
    end_point = trendTime + N_ref2      # å‚è€ƒä¿¡å·ç»“æŸæ—¶é—´ç‚¹

    #æ ¹æ®è¯é¢˜è¯é¢˜æˆä¸ºçƒ­ç‚¹æ—¶é—´ï¼Œç”Ÿæˆå¾®åšå­˜æ”¾è·¯å¾„
    timeStruct = time.strptime(topic_trended_time, '%Y-%m-%d %H:%M')  # å°†æˆä¸ºçƒ­ç‚¹æ—¶é—´ç‚¹è½¬åŒ–ä¸ºæ—¶é—´å…ƒç»„ç±»å‹
    date_path = time.strftime("%Y%m%d", timeStruct)  # å°†æ—¶é—´è½¬åŒ–ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œå¦‚â€œ201712220â€
    blog_filepath=os.path.join(blog_basepath,date_path)
    print "éœ€è¯»å–çš„å¾®åšå­˜æ”¾è·¯å¾„:",blog_filepath
    #ä»å¾®åšå­˜æ”¾è·¯å¾„ä¸­è·å–è¯é¢˜ç›¸å…³çš„å¾®åš
    gts = GenerateTopicSignals()
    blog_lst=gts.collect_blogs_for_one_topic_from_fold(topic=trended_topic,
                                                       blog_filepath=blog_filepath,
                                                       start_point=start_point,
                                                       end_point=end_point)

    #å°†è¯é¢˜ç›¸å…³å¾®åšä»¥jsonçš„å½¢å¼ä¿å­˜
    filepath=os.path.join(blog_basepath,blog_savepath)
    mkdir(filepath)
    filename=os.path.join(filepath,topic_id+'.json')
    print "è¯é¢˜ç›¸å…³å¾®åšä¿å­˜è·¯å¾„ï¼š",filename
    store_json(data=blog_lst,filename=filename)

    #æ ¹æ®æ”¶é›†åˆ°çš„å¾®åšï¼Œå¾—åˆ°çƒ­ç‚¹è¯é¢˜çš„å‚è€ƒä¿¡å·
    #å¾—åˆ°åŸå§‹ä¿¡å·
    original_signals=gts.generate_signals(blog_lst=blog_lst,
                                          start_point=start_point,
                                          signal_length=ref_signal_length,
                                          signal_span=signal_span)
    #å¯¹åŸå§‹ä¿¡å·è¿›è¡Œå¤„ç†
    signals_b, signals_b_s, signals_b_s_c, signals_b_s_c_l=gts.process_signals(original_signal=original_signals,
                                                                               beta=1,
                                                                               alpha=1,
                                                                               N_smooth=10,
                                                                               b_flag=True)
    signal_dict={}
    signal_dict['original_signals']=original_signals
    signal_dict['signals_b'] = signals_b
    signal_dict['signals_b_s'] = signals_b_s
    signal_dict['signals_b_s_c'] = signals_b_s_c
    signal_dict['signals_b_s_c_l'] = signals_b_s_c_l
    #å°†è¯é¢˜ä¿¡å·ä¿å­˜èµ·æ¥
    filepath = os.path.join(blog_basepath, signal_savepath)
    mkdir(filepath)
    filename = os.path.join(filepath, topic_id + '.json')
    print "è¯é¢˜ä¿¡å·ä¿å­˜è·¯å¾„ï¼š",filename
    store_json(data=signal_dict, filename=filename)
"""
#ä¸ºè¯é¢˜äº§ç”Ÿè§‚æµ‹ä¿¡å·,è§‚æµ‹ä¿¡å·é•¿åº¦è¦é•¿äºå‚è€ƒä¿¡å·
#æ­¤æ—¶topic_trended_timeä¸ºåŸºå‡†ç‚¹æ—¶é—´,å½¢å¼ï¼šâ€˜2017-12-28 13:43â€™
#blog_basepathçš„å½¢å¼ä¸ºï¼š'/data0/shaojie5/source_mblog/data/mblogâ€™
#blog_savepathä¸ºï¼šâ€˜obs_topic_related_blogsâ€™
#signal_savepathä¸ºï¼š'obs_topic_signals'
"""
def generate_observation_signals_for_topic(signal_span,
                                           topic_trended_time,
                                           blog_basepath,
                                           trended_topic,
                                           blog_savepath,
                                           topic_id,
                                           signal_savepath,
                                           topic_type=1):

    print "å¼€å§‹æ”¶é›†***",trended_topic,"***çš„ä¿¡å·"
    N_ref1 = 10 * 3600      # è¯é¢˜å¾®åšæ”¶é›†åŸºå‡†ç‚¹ä¹‹å‰ï¼š10å°æ—¶(å¯ä¿®æ”¹)
    N_ref2 = 5 * 3600       # è¯é¢˜å¾®åšæ”¶é›†åŸºå‡†ç‚¹ä¹‹åï¼š3å°æ—¶ï¼ˆå¯ä¿®æ”¹ï¼‰
    obs_signal_length = (N_ref1 + N_ref2) / signal_span  #è§‚æµ‹ä¿¡å·çš„é•¿åº¦
    print "å‚è€ƒä¿¡å·çš„é•¿åº¦ä¸ºï¼š", obs_signal_length

    trendTime = time.mktime(time.strptime(topic_trended_time, '%Y-%m-%d %H:%M'))  # è·å–è¯é¢˜æˆä¸ºçƒ­ç‚¹çš„æ—¶é—´æˆ³æµ®ç‚¹æ•°è¡¨ç¤º
    start_point = trendTime - N_ref1  # å‚è€ƒä¿¡å·å¼€å§‹æ—¶é—´ç‚¹
    end_point = trendTime + N_ref2  # å‚è€ƒä¿¡å·ç»“æŸæ—¶é—´ç‚¹
    print "åŸºå‡†æ—¶é—´ä¸ºï¼š",trendTime ,'==',time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(trendTime))
    print "ä¿¡å·å¼€å§‹æ—¶é—´ç‚¹:",start_point,'==',time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(start_point))
    print "ä¿¡å·ç»“æŸæ—¶é—´ç‚¹:",end_point,'==',time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(end_point))

    # æ ¹æ®åŸºå‡†ç‚¹æ—¶é—´ï¼Œç”Ÿæˆå¾®åšå­˜æ”¾è·¯å¾„
    timeStruct = time.strptime(topic_trended_time, '%Y-%m-%d %H:%M')  # å°†æˆä¸ºçƒ­ç‚¹æ—¶é—´ç‚¹è½¬åŒ–ä¸ºæ—¶é—´å…ƒç»„ç±»å‹
    date_path = time.strftime("%Y%m%d", timeStruct)  # å°†æ—¶é—´è½¬åŒ–ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œå¦‚â€œ201712220â€
    blog_filepath = os.path.join(blog_basepath, date_path)
    print "éœ€è¯»å–çš„å¾®åšå­˜æ”¾è·¯å¾„:", blog_filepath

    # ä»å¾®åšå­˜æ”¾è·¯å¾„ä¸­è·å–è¯é¢˜ç›¸å…³çš„å¾®åš
    gts = GenerateTopicSignals()
    if topic_type:
        blog_lst = gts.collect_blogs_for_one_topic_from_fold(topic=trended_topic,
                                                             blog_filepath=blog_filepath,
                                                             start_point=start_point,
                                                             end_point=end_point)
    else:
        blog_lst = gts.collect_blogs_for_one_topic_from_fold_v1(topic=trended_topic,
                                                             blog_filepath=blog_filepath,
                                                             start_point=start_point,
                                                             end_point=end_point)
    #
    # å°†è¯é¢˜ç›¸å…³å¾®åšä»¥jsonçš„å½¢å¼ä¿å­˜
    filepath = os.path.join(blog_basepath, blog_savepath)
    mkdir(filepath)
    filename = os.path.join(filepath, topic_id + '.json')
    print "è¯é¢˜ç›¸å…³å¾®åšä¿å­˜è·¯å¾„ï¼š", filename
    store_json(data=blog_lst, filename=filename)

    # æ ¹æ®æ”¶é›†åˆ°çš„å¾®åšï¼Œå¾—åˆ°çƒ­ç‚¹è¯é¢˜çš„å‚è€ƒä¿¡å·
    # å¾—åˆ°åŸå§‹ä¿¡å·
    original_signals = gts.generate_signals(blog_lst=blog_lst,
                                            start_point=start_point,
                                            signal_length=obs_signal_length,
                                            signal_span=signal_span)
    # å¯¹åŸå§‹ä¿¡å·è¿›è¡Œå¤„ç†
    signals_b, signals_b_s, signals_b_s_c, signals_b_s_c_l = gts.process_signals(original_signal=original_signals,
                                                                                 beta=1,
                                                                                 alpha=1,
                                                                                 N_smooth=10,
                                                                                 b_flag=True)
    signal_dict = {}
    signal_dict['original_signals'] = original_signals
    signal_dict['signals_b'] = signals_b
    signal_dict['signals_b_s'] = signals_b_s
    signal_dict['signals_b_s_c'] = signals_b_s_c
    signal_dict['signals_b_s_c_l'] = signals_b_s_c_l
    # å°†è¯é¢˜ä¿¡å·ä¿å­˜èµ·æ¥
    filepath = os.path.join(blog_basepath, signal_savepath)
    mkdir(filepath)
    filename = os.path.join(filepath, topic_id + '.json')
    print "è¯é¢˜ä¿¡å·ä¿å­˜è·¯å¾„ï¼š", filename
    store_json(data=signal_dict, filename=filename)

"""
#è¯»å–å‚è€ƒä¿¡å·å¯¹åº”çš„è¯é¢˜
#ä¸‰ç§è¯é¢˜
#1ã€æ­£æ ·æœ¬è¯é¢˜å³çƒ­ç‚¹è¯é¢˜'pos_topic.txt'
#2ã€è´Ÿæ ·æœ¬è¯é¢˜å³éçƒ­ç‚¹è¯é¢˜'neg_topic.txt'
#3ã€è§‚æµ‹è¯é¢˜å³å¾…é¢„æµ‹è¯é¢˜'obs_topic.txt'
"""
def read_topics(blog_basepath,filename):
    filename=os.path.join(blog_basepath,filename)
    topic_lst=[]
    for line in open(filename).readlines():
        line = line.decode('utf8')
        items = line.strip().strip('\n').split(',')
        if len(items)!=3:
            continue
        dict={}
        dict['topic_id']=items[0].encode('utf-8')
        dict['trended_topic'] = items[1].encode('utf-8')
        dict['topic_trended_time'] = items[2].encode('utf-8')
        trendTime = time.mktime(time.strptime(dict['topic_trended_time'], '%Y-%m-%d %H:%M'))  # è·å–è¯é¢˜æˆä¸ºçƒ­ç‚¹çš„æ—¶é—´æˆ³æµ®ç‚¹æ•°è¡¨ç¤º
        print trendTime
        topic_lst.append(dict)
        for key,value in dict.items():
            print key,":",value,type(value)           #ç±»å‹ä¸ºunicode
    return topic_lst

#å¤„ç†ç¨‹åº1:äº§ç”Ÿå‚è€ƒä¿¡å·
def process1(topic_dict):

    print "======================================================================="
    print "\n"
    start_time=datetime.datetime.now()
    print "ç¨‹åºå¼€å§‹æ—¶é—´ï¼š",start_time
    generate_refrence_signals_for_trended_topic(blog_basepath='/data0/shaojie5/source_mblog/data/mblog',
                                                blog_savepath='pos_topic_related_blogs',
                                                signal_savepath='pos_topic_signals',
                                                trended_topic=topic_dict['trended_topic'],
                                                topic_trended_time=topic_dict['topic_trended_time'],
                                                topic_id=topic_dict['topic_id'],
                                                signal_span=600)
    end_time=datetime.datetime.now()
    print "ç¨‹åºç»“æŸæ—¶é—´ï¼š", end_time
    print "ä»£ç è¿è¡Œæ—¶é—´ä¸ºï¼š",(end_time - start_time).seconds

# å¤„ç†ç¨‹åº2:äº§ç”Ÿè§‚æµ‹ä¿¡å·
def process2(topic_dict,topic_type):
    print "======================================================================="
    print "\n"
    start_time = datetime.datetime.now()
    print "ç¨‹åºå¼€å§‹æ—¶é—´ï¼š", start_time
    generate_observation_signals_for_topic(signal_span=600,
                                              topic_trended_time=topic_dict['topic_trended_time'],
                                              blog_basepath='/data0/shaojie5/source_mblog/data/mblog',
                                              trended_topic=topic_dict['trended_topic'],
                                              blog_savepath='obs_topic_related_blogs',
                                              topic_id=topic_dict['topic_id'],
                                              signal_savepath='obs_topic_signals',
                                              topic_type=topic_type)
    end_time = datetime.datetime.now()
    print "ç¨‹åºç»“æŸæ—¶é—´ï¼š", end_time
    print "ä»£ç è¿è¡Œæ—¶é—´ä¸ºï¼š", (end_time - start_time).seconds

#å•çº¿ç¨‹å¤„ç†
if __name__=='__main__':
    pass
    topic_lst = read_topics(blog_basepath='/data0/shaojie5/source_mblog/data/mblog', filename='obs_topic_2018_03_02.txt')
    # # äº§ç”Ÿå‚è€ƒä¿¡å·
    # for topic_dict in topic_lst[41:48]:
    #    process1(topic_dict)

    # äº§ç”Ÿè§‚æµ‹ä¿¡å·(çƒ­æœè¯é¢˜)
    # topic_lst1=topic_lst[82:92]
    # for topic_dict in topic_lst1:
    #     print "å¼€å§‹æ”¶é›†è¯é¢˜ï¼š",topic_dict['trended_topic']
    #     process2(topic_dict,topic_type=1)

    #äº§ç”Ÿè§‚æµ‹ä¿¡å·(è‡ªé€‰è¯é¢˜)
    topic_lst2=topic_lst[110:114]
    for topic_dict in topic_lst2:
        print "å¼€å§‹æ”¶é›†è¯é¢˜ï¼š", topic_dict['trended_topic']
        process2(topic_dict, topic_type=0)






